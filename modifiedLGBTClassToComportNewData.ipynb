{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy.matlib\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten,Input,concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('train_fNxu4vz.csv')\n",
    "data_sub = pd.read_csv('test_fjtUOL8.csv')\n",
    "#data_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scalerL = preprocessing.MinMaxScaler()\n",
    "data_train['Loan_Amount_Requested'] = min_max_scalerL.fit_transform( data_train['Loan_Amount_Requested'].str.replace(',','').astype(int).values.reshape(-1, 1) )\n",
    "\n",
    "data_sub['Loan_Amount_Requested'] = min_max_scalerL.transform( data_sub['Loan_Amount_Requested'].str.replace(',','').astype(int).values.reshape(-1, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scalerA = preprocessing.MinMaxScaler()\n",
    "data_train['Annual_Income'] = min_max_scalerA.fit_transform( data_train['Annual_Income'].fillna(-1).values.reshape(-1, 1) )\n",
    "\n",
    "data_sub['Annual_Income'] = min_max_scalerA.transform( data_sub['Annual_Income'].fillna(-1).values.reshape(-1, 1) )\n",
    "\n",
    "min_max_scalerD = preprocessing.MinMaxScaler()\n",
    "data_train['Debt_To_Income'] = min_max_scalerD.fit_transform( data_train['Debt_To_Income'].fillna(-1).values.reshape(-1, 1) )\n",
    "\n",
    "data_sub['Debt_To_Income'] = min_max_scalerD.transform( data_sub['Debt_To_Income'].fillna(-1).values.reshape(-1, 1) )\n",
    "\n",
    "min_max_scalerI = preprocessing.MinMaxScaler()\n",
    "data_train['Inquiries_Last_6Mo'] = min_max_scalerI.fit_transform(  data_train['Inquiries_Last_6Mo'].fillna(-1).values.reshape(-1, 1) )\n",
    "\n",
    "data_sub['Inquiries_Last_6Mo'] = min_max_scalerI.transform(  data_sub['Inquiries_Last_6Mo'].fillna(-1).values.reshape(-1, 1) )\n",
    "\n",
    "min_max_scalerM = preprocessing.MinMaxScaler()\n",
    "data_train['Months_Since_Deliquency'] = min_max_scalerM.fit_transform( data_train['Months_Since_Deliquency'].fillna(-1).values.reshape(-1, 1))\n",
    "\n",
    "data_sub['Months_Since_Deliquency'] = min_max_scalerM.transform( data_sub['Months_Since_Deliquency'].fillna(-1).values.reshape(-1, 1))\n",
    "\n",
    "min_max_scalerN = preprocessing.MinMaxScaler()\n",
    "data_train['Number_Open_Accounts'] = min_max_scalerN.fit_transform( data_train['Number_Open_Accounts'].fillna(-1).values.reshape(-1, 1) )\n",
    "\n",
    "data_sub['Number_Open_Accounts'] = min_max_scalerN.transform( data_sub['Number_Open_Accounts'].fillna(-1).values.reshape(-1, 1) )\n",
    "\n",
    "min_max_scalerT = preprocessing.MinMaxScaler()\n",
    "data_train['Total_Accounts'] = min_max_scalerT.fit_transform(  data_train['Total_Accounts'].fillna(-1).values.reshape(-1, 1) )\n",
    "\n",
    "data_sub['Total_Accounts'] = min_max_scalerT.transform(  data_sub['Total_Accounts'].fillna(-1).values.reshape(-1, 1) )\n",
    "\n",
    "# gender\n",
    "data_train['Gender'] =  data_train['Gender'].str.replace('Male','1')\n",
    "data_train['Gender'] =  data_train['Gender'].str.replace('Female','0')\n",
    "data_train['Gender']  = data_train['Gender'] .astype(int)\n",
    "\n",
    "data_sub['Gender'] =  data_sub['Gender'].str.replace('Male','1')\n",
    "data_sub['Gender'] =  data_sub['Gender'].str.replace('Female','0')\n",
    "data_sub['Gender']  = data_sub['Gender'] .astype(int)\n",
    "\n",
    "di = {'not verified': 0, 'VERIFIED - income': 0.5,'VERIFIED - income source':1 }\n",
    "\n",
    "data_train['Income_Verified'] = data_train['Income_Verified'].map(di)\n",
    "\n",
    "data_sub['Income_Verified'] = data_sub['Income_Verified'].map(di)\n",
    "\n",
    "dit = {'< 1 year': 0, '1 year': 0.1,'2 years': 0.2,'3 years': 0.3,'4 years': 0.4,'5 years': 0.5,'6 years': 0.6,'7 years': 0.7,'8 years': 0.8,'9 years': 0.9,'10+ years': 1}\n",
    "\n",
    "data_train['Length_Employed'] = data_train['Length_Employed'].map(dit)\n",
    "\n",
    "data_train['Length_Employed'] = data_train['Length_Employed'].fillna(-1)\n",
    "\n",
    "data_sub['Length_Employed'] = data_sub['Length_Employed'].map(dit)\n",
    "\n",
    "data_sub['Length_Employed'] = data_sub['Length_Employed'].fillna(-1)\n",
    "\n",
    "dih = {'Own': 1, 'Mortgage': 0.75,'Rent': 0.5,'Other':0.25,'None':0}\n",
    "\n",
    "data_train['Home_Owner'] = data_train['Home_Owner'].map(dih)\n",
    "data_train['Home_Owner'] = data_train['Home_Owner'].fillna(-1)\n",
    "\n",
    "data_sub['Home_Owner'] = data_sub['Home_Owner'].map(dih)\n",
    "data_sub['Home_Owner'] = data_sub['Home_Owner'].fillna(-1)\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "data_train[\"Purpose_Of_Loan\"] = lb_make.fit_transform(data_train[\"Purpose_Of_Loan\"])\n",
    "\n",
    "data_sub[\"Purpose_Of_Loan\"] = lb_make.transform(data_sub[\"Purpose_Of_Loan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5,random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelee' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e03f7558e774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX2_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelee' is not defined"
     ]
    }
   ],
   "source": [
    "X = data_train[['Loan_Amount_Requested','Annual_Income','Debt_To_Income','Inquiries_Last_6Mo','Months_Since_Deliquency','Number_Open_Accounts','Total_Accounts','Gender','Income_Verified','Length_Employed','Home_Owner']].values\n",
    "y = data_train['Interest_Rate'].values\n",
    "X2 = data_train[['Purpose_Of_Loan']].values\n",
    "\n",
    "\n",
    "P = data_sub[['Loan_Amount_Requested','Annual_Income','Debt_To_Income','Inquiries_Last_6Mo','Months_Since_Deliquency','Number_Open_Accounts','Total_Accounts','Gender','Income_Verified','Length_Employed','Home_Owner']].values\n",
    "P2 = data_sub[['Purpose_Of_Loan']].values\n",
    "\n",
    "b = [] # append results\n",
    "                \n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index,:], X[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X2_train,X2_test = X2[train_index], X2[test_index]\n",
    "    \n",
    "    # training lgbtm\n",
    "    \n",
    "    \n",
    "    out = modelee.predict([X2_test,X_test])\n",
    "    out2 = np.argmax(out,axis=1)\n",
    "    print(f1_score(y_test, out2+1, average='weighted'))\n",
    "    \n",
    "    out = modelee.predict([P2,P])\n",
    "    \n",
    "    b.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EvoTreeOptimization(object):\n",
    "    '''This class stores evolutionary optimization algorithms to trees based models'''\n",
    "    \n",
    "     # inicilaization function\n",
    "    def __init__(self, cost_func, bounds, popsize=10, mutate=0.5, recombination=0.7, maxiter=2000):\n",
    "        self.cost_func=cost_func\n",
    "        self.bounds=bounds\n",
    "        self.popsize=popsize\n",
    "        self.mutate=mutate\n",
    "        self.recombination=recombination\n",
    "        self.maxiter=maxiter \n",
    "        self.bestInd = []\n",
    "        self.meanpop = []\n",
    "        self.best = []\n",
    "        self.globvar = 0 # variable created to tune the output predicted value\n",
    "        \n",
    "        self.objective = 'xentropy'\n",
    "        self.num_class = 1 \n",
    "        self.f = self.lgbm_tra # function to be usded depending on multiclass/singleClass\n",
    "           \n",
    "\n",
    "    def ensure_bounds(self,vec, bounds):\n",
    "        vec_new = []\n",
    "        # cycle through each variable in vector\n",
    "        for i in range(len(vec)):\n",
    "\n",
    "            # variable exceedes the minimum boundary\n",
    "            if vec[i] < bounds[i][0]:\n",
    "                vec_new.append(bounds[i][0])\n",
    "\n",
    "            # variable exceedes the maximum boundary\n",
    "            if vec[i] > bounds[i][1]:\n",
    "                vec_new.append(bounds[i][1])\n",
    "\n",
    "            # the variable is fine\n",
    "            if bounds[i][0] <= vec[i] <= bounds[i][1]:\n",
    "                vec_new.append(vec[i])\n",
    "\n",
    "        return vec_new\n",
    "    \n",
    "    def plotEvolution(self):\n",
    "        plt.plot(trainX.meanpop)\n",
    "        plt.plot(trainX.bestInd)\n",
    "        plt.ylabel('cost function' + str(cost_func) )\n",
    "        plt.ylabel('epochs')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def earlyFunc(self,preds, train_data):\n",
    "        labels = train_data.get_label()\n",
    "        return 'error',  self.cost_func(labels,np.round(preds-self.globvar)), True\n",
    "\n",
    "    def lgbm_tra(self,x,X,y):\n",
    "        \n",
    "        # seleciona os parametros para treinamento\n",
    "        params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': self.objective,\n",
    "            'num_leaves': int(x[1]),      # 31\n",
    "            'learning_rate': x[2],   #0.01,\n",
    "            'feature_fraction': x[3],#0.9,\n",
    "            'bagging_fraction': x[4],#0.8,\n",
    "            'bagging_freq': 1,\n",
    "            'max_depth': int(x[5]),        #-1,\n",
    "            'min_data_in_leaf': int(x[6]), #20,\n",
    "            'lambda_l2': x[7],        # 0,\n",
    "            'is_unbalance' : True,\n",
    "            'max_bin' : int(x[9]),\n",
    "            'num_class' : self.num_class,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        num_k = 5\n",
    "        acc = 0\n",
    "        # treina a arvore\n",
    "        skf = StratifiedKFold(n_splits=num_k,random_state=21)\n",
    "\n",
    "        self.globvar = x[8]\n",
    "\n",
    "        for train_index, test_index in skf.split(X,y):\n",
    "           #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "           X_train, X_test = X[train_index,:], X[test_index,:]\n",
    "           y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "           lgb_train = lgb.Dataset(X_train, y_train)\n",
    "           \n",
    "           print(X_train.shape)            \n",
    "           print(y_train.shape)\n",
    "        \n",
    "           lgb_test = lgb.Dataset(X_test, y_test)\n",
    "            \n",
    "           print(X_test.shape)            \n",
    "           print(y_test.shape)\n",
    "\n",
    "           gbm = lgb.train(params, lgb_train, num_boost_round=int(x[0]),valid_sets = [lgb_test],early_stopping_rounds=30,feval = self.earlyFunc, verbose_eval= 0)     #x[0]\n",
    "\n",
    "           y_pred = gbm.predict(X_test,num_iteration=gbm.best_iteration)\n",
    "           acc += f1_score(y_test,np.round(y_pred-x[8]) ) # F1 score\n",
    "\n",
    "        return (acc/num_k)\n",
    "\n",
    "    def lgbm_tra_multi(self,x,X,y):\n",
    "        \n",
    "        # seleciona os parametros para treinamento\n",
    "        params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': self.objective,\n",
    "            'num_leaves': int(x[1]),      # 31\n",
    "            'learning_rate': x[2],   #0.01,\n",
    "            'feature_fraction': x[3],#0.9,\n",
    "            'bagging_fraction': x[4],#0.8,\n",
    "            'bagging_freq': 1,\n",
    "            'max_depth': int(x[5]),        #-1,\n",
    "            'min_data_in_leaf': int(x[6]), #20,\n",
    "            'lambda_l2': x[7],        # 0,\n",
    "            'is_unbalance' : True,\n",
    "            'max_bin' : int(x[9]),\n",
    "            'num_class' : self.num_class,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        num_k = 5\n",
    "        acc = 0\n",
    "        # treina a arvore\n",
    "        skf = StratifiedKFold(n_splits=num_k)#,random_state=21)\n",
    "\n",
    "        self.globvar = x[8]\n",
    "\n",
    "        for train_index, test_index in skf.split(X,y):\n",
    "           #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "           X_train, X_test = X[train_index,:], X[test_index,:]\n",
    "           y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "           lgb_train = lgb.Dataset(X_train, y_train)           \n",
    "        \n",
    "           lgb_test = lgb.Dataset(X_test, y_test)            \n",
    "        \n",
    "\n",
    "           gbm = lgb.train(params, lgb_train, num_boost_round=int(x[0]),valid_sets = [lgb_test],early_stopping_rounds=30, verbose_eval= 0)     #x[0]\n",
    "\n",
    "           y_pred = gbm.predict(X_test,num_iteration=gbm.best_iteration)\n",
    "           acc += f1_score(y_test,np.argmax(y_pred,axis=1) ,average='weighted') # F1 score\n",
    "\n",
    "        return (acc/num_k)\n",
    "\n",
    "\n",
    "    def run(self,x_train,y_train):\n",
    "        # --- INITIALIZE A POPULATION (step #1) ----------------+\n",
    "        \n",
    "        # check if is multiclass or single class\n",
    "        \n",
    "        if (max(y_train)) > 1:\n",
    "            \n",
    "            self.objective = 'multiclass'\n",
    "            self.num_class = len(set(y_train)) \n",
    "            self.f = self.lgbm_tra_multi\n",
    "        \n",
    "        population = []\n",
    "        for i in range(0, self.popsize):\n",
    "            indv = []\n",
    "            for j in range(len(self.bounds)):\n",
    "                indv.append(random.uniform(self.bounds[j][0], self.bounds[j][1]))\n",
    "            population.append(indv)\n",
    "\n",
    "        print('First gen ...')\n",
    "        scorePop = np.zeros(self.popsize)\n",
    "        for num, ind in enumerate(population):\n",
    "            scorePop[num] = self.f(ind, x_train, y_train)\n",
    "            print(scorePop[num])\n",
    "\n",
    "        # --- SOLVE --------------------------------------------+\n",
    "\n",
    "        # cycle through each generation (step #2)\n",
    "        for i in range(1, self.maxiter + 1):\n",
    "            print('GENERATION:', i)\n",
    "\n",
    "            gen_scores = []  # score keeping\n",
    "\n",
    "            # cycle through each individual in the population\n",
    "            for j in range(0, self.popsize):\n",
    "\n",
    "                # --- MUTATION (step #3.A) ---------------------+\n",
    "\n",
    "                # select three random vector index positions [0, popsize), not including current vector (j)\n",
    "                candidates = np.arange(0, self.popsize)\n",
    "                candidates = np.delete(candidates, j)\n",
    "\n",
    "                random_index = np.random.permutation(candidates)\n",
    "\n",
    "                x_1 = population[random_index[0]]\n",
    "                x_2 = population[random_index[1]]\n",
    "                x_3 = population[random_index[2]]\n",
    "                x_t = population[j]  # target individual\n",
    "\n",
    "                # subtract x3 from x2, and create a new vector (x_diff)\n",
    "                x_diff = [x_2_i - x_3_i for x_2_i, x_3_i in zip(x_2, x_3)]\n",
    "\n",
    "                # multiply x_diff by the mutation factor (F) and add to x_1\n",
    "                v_donor = [x_1_i + self.mutate * x_diff_i for x_1_i, x_diff_i in zip(x_1, x_diff)]\n",
    "                v_donor = self.ensure_bounds(v_donor, self.bounds)\n",
    "\n",
    "                # --- RECOMBINATION (step #3.B) ----------------+\n",
    "\n",
    "                v_trial = []\n",
    "                for k in range(len(x_t)):\n",
    "                    crossover = random.random()\n",
    "                    if crossover <= self.recombination:\n",
    "                        v_trial.append(v_donor[k])\n",
    "\n",
    "                    else:\n",
    "                        v_trial.append(x_t[k])\n",
    "\n",
    "                # --- GREEDY SELECTION (step #3.C) -------------+\n",
    "\n",
    "                score_trial = self.f(v_trial,x_train,y_train)\n",
    "                score_target = scorePop[j]\n",
    "\n",
    "                if score_trial > score_target:\n",
    "                    population[j] = v_trial\n",
    "                    scorePop[j] = score_trial\n",
    "                    #print(score_target, '   <', score_trial, v_trial)\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "                    #print(score_target,'   >', score_target, x_t)\n",
    "\n",
    "\n",
    "            # --- SCORE KEEPING --------------------------------+\n",
    "            gen_avg = np.mean(scorePop)  # current generation avg. fitness\n",
    "            gen_best = max(scorePop)  # fitness of best individual\n",
    "            gen_sol = population[np.argmax(scorePop)]  # solution of best individual\n",
    "\n",
    "            print('      > GENERATION AVERAGE:', gen_avg)\n",
    "            print('      > GENERATION BEST:', gen_best)\n",
    "            print('         > BEST SOLUTION:', gen_sol, '\\n')\n",
    "            \n",
    "            self.bestInd.append(gen_best)\n",
    "            self.meanpop.append(gen_avg)\n",
    "            self.best = gen_sol\n",
    "\n",
    "        return gen_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_func = f1_score  # Cost function\n",
    "#cost_func = accuracy_score\n",
    "bounds = [(200, 1000),(15,50), (0.001,0.1 ), (0.5, 1),(0.7, 1), (6, 30),(10, 30),(0,20),(-0.4,0.4),(32,128)]  # Bounds [(x1_min, x1_max), (x2_min, x2_max),...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = EvoTreeOptimization(cost_func,bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First gen ...\n",
      "0.5183525181290426\n",
      "0.5096427284034795\n",
      "0.48326378819882565\n",
      "0.5182791643698614\n",
      "0.5152553541376099\n",
      "0.357825212213501\n",
      "0.5174564846731868\n",
      "0.5181440368982836\n",
      "0.5177309869373563\n",
      "0.5130813625216508\n",
      "GENERATION: 1\n",
      "      > GENERATION AVERAGE: 0.5170125512241222\n",
      "      > GENERATION BEST: 0.5190501452672487\n",
      "         > BEST SOLUTION: [549.1552690784881, 34.88364076246671, 0.057373210427928464, 0.7395869821604049, 0.9887678499600507, 15.86924892599761, 10, 6.875431112453276, 0.31725283764028034, 83.76960028479517] \n",
      "\n",
      "GENERATION: 2\n",
      "      > GENERATION AVERAGE: 0.5178857450270831\n",
      "      > GENERATION BEST: 0.5197804394880073\n",
      "         > BEST SOLUTION: [537.2847219601083, 34.88364076246671, 0.07082028710527981, 0.7395869821604049, 0.8344131095396432, 15.86924892599761, 13.6341124874729, 6.875431112453276, 0.04451060507009391, 90.57013107877972] \n",
      "\n",
      "GENERATION: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-dfdb4191c55e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-8c49089b4df3>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, x_train, y_train)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# --- GREEDY SELECTION (step #3.C) -------------+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0mscore_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0mscore_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorePop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-8c49089b4df3>\u001b[0m in \u001b[0;36mlgbm_tra_multi\u001b[0;34m(self, x, X, y)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m            \u001b[0mgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlgb_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m#x[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m            \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1974\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1975\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1977\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainX.run(X_train,y_train-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiclass'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131447"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "78870/26290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-47-a897716a2c68>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-a897716a2c68>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    params = {\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# seleciona os parametros para treinamento\n",
    "        params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': self.objective,\n",
    "            'num_leaves': int(x[1]),      # 31\n",
    "            'learning_rate': x[2],   #0.01,\n",
    "            'feature_fraction': x[3],#0.9,\n",
    "            'bagging_fraction': x[4],#0.8,\n",
    "            'bagging_freq': 1,\n",
    "            'max_depth': int(x[5]),        #-1,\n",
    "            'min_data_in_leaf': int(x[6]), #20,\n",
    "            'lambda_l2': x[7],        # 0,\n",
    "            'is_unbalance' : True,\n",
    "            'max_bin' : int(x[9]),\n",
    "            'num_class' : self.num_class,\n",
    "            'verbose': -1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_breast_cancer,load_boston,load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "X1=load_wine()\n",
    "df_1=pd.DataFrame(X1.data,columns=X1.feature_names)\n",
    "Y_1=X1.target#Scaling using the Standard Scaler\n",
    "sc_1=StandardScaler()\n",
    "sc_1.fit(df_1)\n",
    "X_1=pd.DataFrame(sc_1.fit_transform(df_1))#train-test-split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_1,Y_1,test_size=0.3,random_state=0)#Converting the dataset in proper LGB format\n",
    "d_train=lgb.Dataset(X_train, label=y_train)#setting up the parameters\n",
    "params={}\n",
    "params['learning_rate']=0.03\n",
    "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "params['objective']='multiclass' #Multi-class target feature\n",
    "params['metric']='multi_logloss' #metric for multi-class\n",
    "params['max_depth']=10\n",
    "params['num_class']=3 #no.of unique values in the target class not inclusive of the end value#training the model\n",
    "clf=lgb.train(params,d_train,100)  #training the model on 100 epocs#prediction on the test dataset\n",
    "y_pred_1=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 13)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleciona os parametros para treinamento\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'bagging_freq': 1,\n",
    "    'is_unbalance' : True,\n",
    "    'num_class' : 3,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train,y_train-1)\n",
    "gbm = lgb.train(params, lgb_train, verbose_eval= 1)     #x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39914341, 0.47282652, 0.12803007],\n",
       "       [0.09478797, 0.33043618, 0.57477585],\n",
       "       [0.20177232, 0.39947447, 0.39875321],\n",
       "       ...,\n",
       "       [0.34032643, 0.53123315, 0.12844042],\n",
       "       [0.17085772, 0.32768506, 0.50145723],\n",
       "       [0.06011081, 0.37240119, 0.56748799]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
